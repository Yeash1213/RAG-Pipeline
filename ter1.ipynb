{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install  bitsandbytes\n",
        "!pip install langchain\n",
        "pip install langchain-community\n",
        "pip install chromadb"
      ],
      "metadata": {
        "id": "HexGvIa-3pWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB08QFla3StA"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from time import time\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader(\"/FinalScraptedData.txt\", encoding=\"utf8\")\n",
        "documents = loader.load()\n",
        "documents"
      ],
      "metadata": {
        "id": "M7XhK2J632yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(documents)\n",
        "splits"
      ],
      "metadata": {
        "id": "s5MLcSlw37Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of splits: {len(splits)}\")\n"
      ],
      "metadata": {
        "id": "sUjZLLx24A3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
      ],
      "metadata": {
        "id": "_5BGpdrf4Ip2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "id": "yczSXJDS4Oee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=\"chroma_db\")"
      ],
      "metadata": {
        "id": "QTWc6MtU4U5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever()\n",
        "retriever"
      ],
      "metadata": {
        "id": "Ukpbx6cC4ep4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "7aTTa74W4ir2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli whoami"
      ],
      "metadata": {
        "id": "l5ipGUGT4tmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-3.1-8B\"\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    token = \"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\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token = \"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\")"
      ],
      "metadata": {
        "id": "7FSgWWA141aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "\n",
        "RESPONSE_TEMPLATE = \"\"\"[INST]\n",
        "<>Context:\n",
        "    {context}\n",
        "\n",
        "INSTRUCTION:\n",
        "    Using the aforementioned contexts, answer the following question in short. DO NOT make things by your own. If any question is irrelevant, say that you DON'T know. But, feel free to answer some general questions.\n",
        "    Do not answer any query related to any other institution other than NSU (North South University).\n",
        "\n",
        "    Question: {question}[/INST]\n",
        "    Helpful Answer:\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "PROMPT = PromptTemplate.from_template(RESPONSE_TEMPLATE)\n",
        "PROMPT = PromptTemplate(template=RESPONSE_TEMPLATE, input_variables=[\"context\", \"question\"])\n",
        "PROMPT"
      ],
      "metadata": {
        "id": "qW9ElhMY43_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_pipeline = transformers.pipeline(\n",
        "        \"text2text-generation\",\n",
        "        max_new_tokens = 200,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",)\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=query_pipeline)"
      ],
      "metadata": {
        "id": "ihWoxZ3D48QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    chain_type='stuff',\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs={\n",
        "        \"verbose\": False,\n",
        "        \"prompt\": PROMPT,\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "v1xcx5YS49D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_rag(qa, query):\n",
        "    result = qa.run(query)\n",
        "    return result"
      ],
      "metadata": {
        "id": "Fx3pPR7B4_qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who are the faculty of NorthSouth University in ece department?\"\n",
        "result = test_rag(qa_chain, query)"
      ],
      "metadata": {
        "id": "TE_YEcwU5Hl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "xPAj2c025Kr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(result, str) and \"Helpful Answer\" in result:\n",
        "    cleaned_answer = result.split(\"Helpful Answer:\", 1)[-1].strip()\n",
        "else:\n",
        "    cleaned_answer = result\n",
        "\n",
        "print(\"\\nAnswer: \", cleaned_answer)"
      ],
      "metadata": {
        "id": "2reIk9Cp5P7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = input(\"Question:\")\n",
        "\n",
        "while query.lower() != 'stop':\n",
        "  result = test_rag(qa_chain, query)\n",
        "\n",
        "  if isinstance(result, str) and \"Helpful Answer\" in result:\n",
        "      cleaned_answer = result.split(\"Helpful Answer:\", 1)[-1].strip()\n",
        "  else:\n",
        "      cleaned_answer = result\n",
        "\n",
        "  print(f\"Answer: {cleaned_answer}\")\n",
        "\n",
        "  query = input(\"Question:\")"
      ],
      "metadata": {
        "id": "qRgQPWgH5VJi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}